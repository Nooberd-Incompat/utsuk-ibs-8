cs-450:

Question: attacks
Answer:

Sources (40):
1. NIST AML-1.pdf (similarity: 0.646)
   Preview: Such attacks have been demonstrated under real-world conditions, and their sophistication and impacts have been increasing steadily. The field of AML is concerned with studying these attacks.
2. NIST AML-1.pdf (similarity: 0.608)
   Preview: There are three main types of attacks: White-box attacks. These assume that the attacker operates with full knowledge about the ML system, including the training data, model architecture, and model hy...
3. NIST AML-1.pdf (similarity: 0.586)
   Preview: Section 2.1 introduces the taxonomy of attacks for PredAI systems, which defines the broad categories of attacker objectives and goals, and identifies the capabilities that an adversary must leverage ...
4. NIST AML-1.pdf (similarity: 0.583)
   Preview: While these attacks operate under very strong assumptions, the main reason for analyzing them is to test the vulnerability of a system against worst-case adversaries and 8 NIST AI 100-2e2025 March 202...
5. NIST AML-1.pdf (similarity: 0.563)
   Preview: White-Box Evasion Attacks . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.2. Black-Box Evasion Attacks . . . . . . . . . . . . . . . . . . . . . . . 15 2.2.3. Transferability of Attacks . . . . ....
6. NIST AML-1.pdf (similarity: 0.559)
   Preview: From there, one may click on the page number shown at the end of the definition to return. This document provides an Index of attack types to easily navigate and reference attacks and corresponding mi...
7. NIST AML-1.pdf (similarity: 0.557)
   Preview: It must consider the capabilities of attackers, the model or system properties that attackers might seek to violate in pursuit of their objectives, and the design of attack methods that exploit vulner...
8. NIST AML-2.pdf (similarity: 0.553)
   Preview: This is a form of FUNCTIONAL ATTACK. An interesting observation is that the universal perturbations gener- alize across deep network architectures, suggesting similarity in the decision boundaries tra...
9. NIST AML-2.pdf (similarity: 0.552)
   Preview: Taxonomy of attacks on GenAI systems Resource ControlQuery Access Training Data Control Model ControlIntegrityResource ControlQuery Access Training DataControlAvailabilityIndirect Prompt InjectionDire...
10. NIST AML-1.pdf (similarity: 0.548)
   Preview: In a DATA POISONING attack 40, 148, an adversary controls a subset of the training data by either inserting or modifying training samples. In a MODEL POISONING attack 222, the adversary controls the m...
11. NIST AML-2.pdf (similarity: 0.546)
   Preview: Evasion attacks in the real world While many of the attacks discussed in this section were demonstrated only in research settings, several evasion attacks have been demonstrated in the real world, and...
12. NIST AML-1.pdf (similarity: 0.545)
   Preview: Evasion Attacks and Mitigations NISTAML.022  Back to Index The discovery of evasion attacks against ML models has led to significant growth in AML research over the last decade. In an evasion attack, ...
13. NIST AML-4.pdf (similarity: 0.544)
   Preview: Trojaning attack on neural networks. In NDSS. The Internet Society, 2018. URL: http:dblp.uni-trier.dedbconfndssndss2018.htmlLiuMALZW018. 223 Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflecti...
14. NIST AML-1.pdf (similarity: 0.541)
   Preview: These attacks assume that the attacker operates with minimal, and sometimes no knowledge at all about the ML system. An adversary might have query access to the model, but they have no other informati...
15. NIST AML-1.pdf (similarity: 0.534)
   Preview: A corresponding set of mitigations for each class of attacks is provided in the attack class sections. Section 3 considers GenAI systems. Section 3.1 introduces the taxonomy of attacks for GenAI syste...
16. NIST AML-3.pdf (similarity: 0.533)
   Preview: A subset of these attacks, in which the main user provides in-context instructions that are appended to higher-trust instructions like those provided by the application designer such as the models SYS...
17. NIST AML-3.pdf (similarity: 0.530)
   Preview: Optimization techniques can then be used to learn attacks, including techniques that follow from attacks designed for PredAI language classifiers e.g., HotFlip 117 and gradient-free techniques that us...
18. NIST AML-2.pdf (similarity: 0.526)
   Preview: 84 introduced backdoor attacks in which the trigger is blended into the training data. Follow-up work introduced the concept of clean-label backdoor attacks 380 in which the adversary can- not change ...
19. NIST AML-2.pdf (similarity: 0.523)
   Preview: Where there are specific types of a more general class of attack for example, a jailbreak is a specific kind of direct prompting attack attack, the specific attack is linked to the more general attack...
20. NIST AML-1.pdf (similarity: 0.523)
   Preview: Evasion attacks require the modification of testing samples to create adversarial examples that are misclassified by the model while often remaining stealthy and imperceptible to humans 38, 144, 362. ...
21. NIST AML-3.pdf (similarity: 0.523)
   Preview: With a focus on direct prompting attacks to enable misuse, we note the following broad categories of direct prompting techniques see 393:  Optimization-based attacks design attack objective functions ...
22. NIST AML-2.pdf (similarity: 0.523)
   Preview: All of the above attacks impact a small set of targeted samples that are selected by the attacker during training, and they have only been tested for continuous image datasets with the exception of St...
23. NIST AML-1.pdf (similarity: 0.520)
   Preview: The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities r...
24. NIST AML-4.pdf (similarity: 0.520)
   Preview: 193 Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, and Aleksander Madry. Rethinking backdoor attacks. In An- dreas Krause, Emma Brunskill, Kyunghyun...
25. NIST AML-1.pdf (similarity: 0.519)
   Preview: Early known instances of evasion attacks date back to 1988 with the work of Kearns and Li 192 and 2004 when Dalvi et al. 98 and Lowd and Meek 226 demonstrated the ex- istence of adversarial examples f...
26. NIST AML-2.pdf (similarity: 0.517)
   Preview: The attack generalizes to all samples in a subpopulation and requires minimal knowledge about the ML model and a small number of poisoned samples proportional to the subpopulation size.
27. NIST AML-2.pdf (similarity: 0.517)
   Preview: Recently, poisoning attacks have gained more attention in industry applications as well 199. They can even be orchestrated at scale so that an adversary with limited financial resources could control ...
28. NIST AML-1.pdf (similarity: 0.516)
   Preview: These attacks are classified according to the following dimensions: 1 learning method and stage of the learning process when the attack is mounted, 2 attacker goals and objectives, 4 NIST AI 100-2e202...
29. NIST AML-1.pdf (similarity: 0.515)
   Preview: Evasion attacks modify testing samples to create ADVERSARIAL EXAMPLE 38, 144, 362, which are similar to the original sample e.g., according to certain distance metrics but alter the model predictions ...
30. NIST AML-2.pdf (similarity: 0.514)
   Preview: The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities r...
31. NIST AML-5.pdf (similarity: 0.512)
   Preview: 283 Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia C...
32. NIST AML-1.pdf (similarity: 0.509)
   Preview: . . . . . . . . . 5 2.1.2. Attacker Goals and Objectives . . . . . . . . . . . . . . . . . . . . . 6 2.1.3. Attacker Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.4. Attacke...
33. NIST AML-5.pdf (similarity: 0.509)
   Preview: Post-training detection of backdoor attacks for two-class and multi-attack scenarios. In The Tenth International Con- ference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. ...
34. NIST AML-4.pdf (similarity: 0.508)
   Preview: In Michael Bailey, Sotiris Ioannidis, Manolis Stamatogiannakis, and Thorsten Holz, editors, Research in At- tacks, Intrusions, and Defenses -21st International Symposium, RAID 2018, Proceed- ings, Lec...
35. NIST AML-3.pdf (similarity: 0.508)
   Preview: Attackers can leverage techniques for indirect prompt injection that are similar to those used in direct prompt injection attacks, such as using a JAILBREAK that allows the attacker to substitute thei...
36. NIST AML-5.pdf (similarity: 0.508)
   Preview: 287 Dario Pasquini, Martin Strohmeier, and Carmela Troncoso. Neural Exec: Learning and learning from execution triggers for prompt injection attacks, 2024. URL: https: arxiv.orgabs2403.03792, arXiv:24...
37. NIST AML-3.pdf (similarity: 0.506)
   Preview: Attackers can take advantage of this fact by offering maliciously designed models, such as pre-trained models that enable a BACKDOOR POISONING ATTACK or TARGETED POISONING AT- TACK.
38. NIST AML-2.pdf (similarity: 0.505)
   Preview: The classification of poisoning attacks in this document is inspired by the framework developed by Cin√† et al. 91, which includes additional refer- ences to poisoning attacks and mitigations. 2.3.1.
39. NIST AML-5.pdf (similarity: 0.504)
   Preview: 58 O out-of-distribution Data that was collected at a different time and possibly under different conditions or in a different environment than the data collected to train the model. 56 P poisoning at...
40. NIST AML-2.pdf (similarity: 0.503)
   Preview: These attacks have been demon- strated for HIDDEN MARKOV MODEL, SUPPORT VECTOR MACHINES 19, FEEDFORWARD NEU- RAL NETWORKS 134, 233, 437, CONVOLUTIONAL NEURAL NETWORKS 361, FEDERATED LEARN- ING 240, GE...

cs-400

Question: attacks
Answer: j

Sources (40):
1. NIST AML-1.pdf (similarity: 0.646)
   Preview: Such attacks have been demonstrated under real-world conditions, and their sophistication and impacts have been increasing steadily. The field of AML is concerned with studying these attacks.
2. NIST AML-1.pdf (similarity: 0.584)
   Preview: Sections 2.2, 2.3, and 2.4 discuss the major classes of attacks: evasion, poisoning, and privacy, respectively. A corresponding set of mitigations for each class of attacks is provided in the attack c...
3. NIST AML-2.pdf (similarity: 0.581)
   Preview: These attacks have been demon- strated for HIDDEN MARKOV MODEL, SUPPORT VECTOR MACHINES 19, FEEDFORWARD NEU- RAL NETWORKS 134, 233, 437, CONVOLUTIONAL NEURAL NETWORKS 361, FEDERATED LEARN- ING 240, GE...
4. NIST AML-2.pdf (similarity: 0.566)
   Preview: These attacks require adversarial access to the model design or training environment and are applicable when model training is outsourced to a more powerful entity, such as a cloud service. Other data...
5. NIST AML-1.pdf (similarity: 0.557)
   Preview: It must consider the capabilities of attackers, the model or system properties that attackers might seek to violate in pursuit of their objectives, and the design of attack methods that exploit vulner...
6. NIST AML-2.pdf (similarity: 0.552)
   Preview: Taxonomy of attacks on GenAI systems Resource ControlQuery Access Training Data Control Model ControlIntegrityResource ControlQuery Access Training DataControlAvailabilityIndirect Prompt InjectionDire...
7. NIST AML-3.pdf (similarity: 0.548)
   Preview: This is often difficult and time-consuming, leading to less rigorous and reliable evaluations of novel mitigations; often they appear very power- ful, but are quickly shown lack robustness to unforese...
8. NIST AML-1.pdf (similarity: 0.548)
   Preview: In a DATA POISONING attack 40, 148, an adversary controls a subset of the training data by either inserting or modifying training samples. In a MODEL POISONING attack 222, the adversary controls the m...
9. NIST AML-2.pdf (similarity: 0.546)
   Preview: Evasion attacks in the real world While many of the attacks discussed in this section were demonstrated only in research settings, several evasion attacks have been demonstrated in the real world, and...
10. NIST AML-4.pdf (similarity: 0.545)
   Preview: 40 Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In Proceedings of the 29th International Coference on International Conference on Machine Learni...
11. NIST AML-2.pdf (similarity: 0.542)
   Preview: This section describes availability poisoning, targeted poisoning, backdoor poisoning, and model poisoning attacks classified according to their adversarial objective. For each poi- soning attack cate...
12. NIST AML-1.pdf (similarity: 0.540)
   Preview: In an evasion attack, the adversarys goal is to generate adversarial examples: samples whose classification can be changed to an arbitrary class of the attackers choice  often with only minimal pertur...
13. NIST AML-5.pdf (similarity: 0.540)
   Preview: 56 P poisoning attacks Adversarial attacks in which an adversary interferes with a model during its TRAINING STAGE, such as by inserting malicious training data DATA POISONING or modifying the trainin...
14. NIST AML-1.pdf (similarity: 0.539)
   Preview: These attacks can be targeted i.e., the adversarial examples class is selected by the attacker or untargeted i.e., the adversarial examples are misclassified to any other incorrect class. Szedegy et a...
15. NIST AML-3.pdf (similarity: 0.539)
   Preview: executing attackers SQL queries. 43 NIST AI 100-2e2025 March 2025 3.3.1. Attack Techniques A range of techniques exist for launching direct prompting attacks, many of which gener- alise across various...
16. NIST AML-2.pdf (similarity: 0.536)
   Preview: 104 identified two main factors that contribute to attack transfer- ability for both evasion and poisoning: the intrinsic adversarial vulnerability of the target model and the complexity of the surrog...
17. NIST AML-1.pdf (similarity: 0.536)
   Preview: Attack Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.1.1. Stages of Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.1.2. Attacker Goals and Ob...
18. NIST AML-1.pdf (similarity: 0.536)
   Preview: Several adver- sarial attack classification frameworks have been introduced in prior works 42, 358, and the goal here is to create a standard terminology for adversarial attacks on ML that unifies exi...
19. NIST AML-1.pdf (similarity: 0.534)
   Preview: . . . . . . . . . 8 2.1.5. Data Modality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.2. Evasion Attacks and Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2.1...
20. NIST AML-4.pdf (similarity: 0.532)
   Preview: Poisoning attacks on cyber attack detectors for industrial control systems. In Proceedings of the 36th Annual ACM Symposium on Applied Computing , SAC 21, page 116125, New York, NY, USA, 2021. Associa...
21. NIST AML-3.pdf (similarity: 0.532)
   Preview: A subset of these attacks, in which the main user provides in-context instructions that are appended to higher-trust instructions like those provided by the application designer such as the models SYS...
22. NIST AML-3.pdf (similarity: 0.530)
   Preview: Optimization techniques can then be used to learn attacks, including techniques that follow from attacks designed for PredAI language classifiers e.g., HotFlip 117 and gradient-free techniques that us...
23. NIST AML-2.pdf (similarity: 0.527)
   Preview: Targeted poisoning attacks are notoriously challenging to defend against. Jagielski et al. 180 showed an impossibility result for subpopulation poisoning attacks.
24. NIST AML-5.pdf (similarity: 0.527)
   Preview: 6, 22, 107 backdoor poisoning attack A poisoning attack that causes a model to perform an adversary- selected behaviour in response to inputs that follow a particular BACKDOOR PAT- TERN. 6, 42 classif...
25. NIST AML-1.pdf (similarity: 0.523)
   Preview: Evasion attacks require the modification of testing samples to create adversarial examples that are misclassified by the model while often remaining stealthy and imperceptible to humans 38, 144, 362. ...
26. NIST AML-3.pdf (similarity: 0.523)
   Preview: With a focus on direct prompting attacks to enable misuse, we note the following broad categories of direct prompting techniques see 393:  Optimization-based attacks design attack objective functions ...
27. NIST AML-1.pdf (similarity: 0.523)
   Preview: Section 2.1 introduces the taxonomy of attacks for PredAI systems, which defines the broad categories of attacker objectives and goals, and identifies the capabilities that an adversary must leverage ...
28. NIST AML-2.pdf (similarity: 0.523)
   Preview: All of the above attacks impact a small set of targeted samples that are selected by the attacker during training, and they have only been tested for continuous image datasets with the exception of St...
29. NIST AML-1.pdf (similarity: 0.520)
   Preview: The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities r...
30. NIST AML-1.pdf (similarity: 0.519)
   Preview: Early known instances of evasion attacks date back to 1988 with the work of Kearns and Li 192 and 2004 when Dalvi et al. 98 and Lowd and Meek 226 demonstrated the ex- istence of adversarial examples f...
31. NIST AML-2.pdf (similarity: 0.517)
   Preview: The attack generalizes to all samples in a subpopulation and requires minimal knowledge about the ML model and a small number of poisoned samples proportional to the subpopulation size.
32. NIST AML-2.pdf (similarity: 0.517)
   Preview: Recently, poisoning attacks have gained more attention in industry applications as well 199. They can even be orchestrated at scale so that an adversary with limited financial resources could control ...
33. NIST AML-1.pdf (similarity: 0.517)
   Preview: These assume that the attacker operates with full knowledge about the ML system, including the training data, model architecture, and model hyperparam- eters. While these attacks operate under very st...
34. NIST AML-5.pdf (similarity: 0.517)
   Preview: 7 data poisoning A POISONING ATTACKS in which an adversary controls part of the training data. 5, 36, 37, 40, 111 data privacy attacks Attacks against machine learning models that extract sensitive in...
35. NIST AML-3.pdf (similarity: 0.516)
   Preview: As in PredAI, attacks may be applicable to a single setting and model, or may instead be uni- versal affecting models on a range of separate queries, see Sec. 2.2.1 andor transferable affecting models...
36. NIST AML-1.pdf (similarity: 0.516)
   Preview: These attacks are classified according to the following dimensions: 1 learning method and stage of the learning process when the attack is mounted, 2 attacker goals and objectives, 4 NIST AI 100-2e202...
37. NIST AML-2.pdf (similarity: 0.514)
   Preview: The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities r...
38. NIST AML-1.pdf (similarity: 0.512)
   Preview: Figure 1 connects each attack class with the capabilities required to mount the attack. For example, backdoor attacks that cause integrity violations require control of the training and testing data t...
39. NIST AML-2.pdf (similarity: 0.511)
   Preview: Where there are specific types of a more general class of attack for example, a jailbreak is a specific kind of direct prompting attack attack, the specific attack is linked to the more general attack...
40. NIST AML-3.pdf (similarity: 0.508)
   Preview: As in PredAI, attackers can also vary in their knowledge of the underlying ML model, from full knowledge of the ML system including model weights white-box attacks, to minimal knowledge and systems wi...

cs-350

Question: attacks
Answer: .

Sources (40):
1. NIST AML-1.pdf (similarity: 0.646)
   Preview: Such attacks have been demonstrated under real-world conditions, and their sophistication and impacts have been increasing steadily. The field of AML is concerned with studying these attacks.
2. NIST AML-3.pdf (similarity: 0.625)
   Preview: An attacker may have a variety of goals when performing these attacks 219, 220, 337, such as to:  Enable misuse. Attackers may use direct prompting attacks to bypass model-level defenses that a model ...
3. NIST AML-3.pdf (similarity: 0.598)
   Preview: Attack Techniques A range of techniques exist for launching direct prompting attacks, many of which gener- alise across various attacker objectives.
4. NIST AML-3.pdf (similarity: 0.584)
   Preview: These attacks often involve disrupting the models behavior in subtle ways that may not be obvious to the end user.
5. NIST AML-2.pdf (similarity: 0.582)
   Preview: Decision-based attacks: In this more restrictive setting, attackers only obtain the fi- nal predicted labels of the model.
6. NIST AML-2.pdf (similarity: 0.581)
   Preview: These attacks have been demon- strated for HIDDEN MARKOV MODEL, SUPPORT VECTOR MACHINES 19, FEEDFORWARD NEU- RAL NETWORKS 134, 233, 437, CONVOLUTIONAL NEURAL NETWORKS 361, FEDERATED LEARN- ING 240, GE...
7. NIST AML-1.pdf (similarity: 0.568)
   Preview: Specific attack classes are also introduced for each type of capability. Sections 2.2, 2.3, and 2.4 discuss the major classes of attacks: evasion, poisoning, and privacy, respectively. A corresponding...
8. NIST AML-1.pdf (similarity: 0.567)
   Preview: Evasion Attacks and Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2.1. White-Box Evasion Attacks . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.2. Black-Box Evasion Attacks ...
9. NIST AML-2.pdf (similarity: 0.566)
   Preview: These attacks require adversarial access to the model design or training environment and are applicable when model training is outsourced to a more powerful entity, such as a cloud service. Other data...
10. NIST AML-3.pdf (similarity: 0.565)
   Preview: Approaches to competing objectives-based attacks include: 1. Prefix injection: This method involves prompting the model to start responses with an affirmative confirmation.
11. NIST AML-4.pdf (similarity: 0.563)
   Preview: Trojaning attack on neural networks. In NDSS. The Internet Society, 2018. URL: http:dblp.uni-trier.dedbconfndssndss2018.htmlLiuMALZW018. 223 Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflecti...
12. NIST AML-1.pdf (similarity: 0.559)
   Preview: From there, one may click on the page number shown at the end of the definition to return. This document provides an Index of attack types to easily navigate and reference attacks and corresponding mi...
13. NIST AML-1.pdf (similarity: 0.557)
   Preview: It must consider the capabilities of attackers, the model or system properties that attackers might seek to violate in pursuit of their objectives, and the design of attack methods that exploit vulner...
14. NIST AML-1.pdf (similarity: 0.553)
   Preview: POISONING ATTACKS 40 occur during the ML training stage. In a DATA POISONING attack 40, 148, an adversary controls a subset of the training data by either inserting or modifying training samples. In a...
15. NIST AML-2.pdf (similarity: 0.552)
   Preview: Taxonomy of attacks on GenAI systems Resource ControlQuery Access Training Data Control Model ControlIntegrityResource ControlQuery Access Training DataControlAvailabilityIndirect Prompt InjectionDire...
16. NIST AML-3.pdf (similarity: 0.548)
   Preview: This is often difficult and time-consuming, leading to less rigorous and reliable evaluations of novel mitigations; often they appear very power- ful, but are quickly shown lack robustness to unforese...
17. NIST AML-2.pdf (similarity: 0.546)
   Preview: Evasion attacks in the real world While many of the attacks discussed in this section were demonstrated only in research settings, several evasion attacks have been demonstrated in the real world, and...
18. NIST AML-1.pdf (similarity: 0.544)
   Preview: While these attacks operate under very strong assumptions, the main reason for analyzing them is to test the vulnerability of a system against worst-case adversaries and 8 NIST AI 100-2e2025 March 202...
19. NIST AML-2.pdf (similarity: 0.542)
   Preview: This section describes availability poisoning, targeted poisoning, backdoor poisoning, and model poisoning attacks classified according to their adversarial objective. For each poi- soning attack cate...
20. NIST AML-1.pdf (similarity: 0.540)
   Preview: In an evasion attack, the adversarys goal is to generate adversarial examples: samples whose classification can be changed to an arbitrary class of the attackers choice  often with only minimal pertur...
21. NIST AML-5.pdf (similarity: 0.540)
   Preview: 56 P poisoning attacks Adversarial attacks in which an adversary interferes with a model during its TRAINING STAGE, such as by inserting malicious training data DATA POISONING or modifying the trainin...
22. NIST AML-1.pdf (similarity: 0.540)
   Preview: Evasion attacks in the real world . . . . . . . . . . . . . . . . . . . . 16 2.2.5. Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.3. Poisoning Attacks and Mitigati...
23. NIST AML-2.pdf (similarity: 0.536)
   Preview: 104 identified two main factors that contribute to attack transfer- ability for both evasion and poisoning: the intrinsic adversarial vulnerability of the target model and the complexity of the surrog...
24. NIST AML-1.pdf (similarity: 0.536)
   Preview: Several adver- sarial attack classification frameworks have been introduced in prior works 42, 358, and the goal here is to create a standard terminology for adversarial attacks on ML that unifies exi...
25. NIST AML-3.pdf (similarity: 0.532)
   Preview: A subset of these attacks, in which the main user provides in-context instructions that are appended to higher-trust instructions like those provided by the application designer such as the models SYS...
26. NIST AML-1.pdf (similarity: 0.531)
   Preview: These attacks can be targeted i.e., the adversarial examples class is selected by the attacker or untargeted i.e., the adversarial examples are misclassified to any other incorrect class. Szedegy et a...
27. NIST AML-4.pdf (similarity: 0.531)
   Preview: 40 Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In Proceedings of the 29th International Coference on International Conference on Machine Learni...
28. NIST AML-3.pdf (similarity: 0.530)
   Preview: Optimization techniques can then be used to learn attacks, including techniques that follow from attacks designed for PredAI language classifiers e.g., HotFlip 117 and gradient-free techniques that us...
29. NIST AML-2.pdf (similarity: 0.527)
   Preview: Targeted poisoning attacks are notoriously challenging to defend against. Jagielski et al. 180 showed an impossibility result for subpopulation poisoning attacks.
30. NIST AML-5.pdf (similarity: 0.527)
   Preview: 6, 22, 107 backdoor poisoning attack A poisoning attack that causes a model to perform an adversary- selected behaviour in response to inputs that follow a particular BACKDOOR PAT- TERN. 6, 42 classif...
31. NIST AML-2.pdf (similarity: 0.525)
   Preview: 375 described a methodology for designing adaptive attacks against proposed defenses and circumvented 13 existing defenses. They advocate for de- signing adaptive attacks to test newly proposed defens...
32. NIST AML-2.pdf (similarity: 0.524)
   Preview: Interestingly, a manual analysis of these adversarial examples revealed that attackers do not employ optimization-based attacks, but rather utilize relatively simple methods for evasion, such as image...
33. NIST AML-1.pdf (similarity: 0.523)
   Preview: Deployment-time attacks. Other types of attacks can be mounted against deployed mod- els. Evasion attacks modify testing samples to create ADVERSARIAL EXAMPLE 38, 144, 362, which are similar to the or...
34. NIST AML-2.pdf (similarity: 0.523)
   Preview: All of the above attacks impact a small set of targeted samples that are selected by the attacker during training, and they have only been tested for continuous image datasets with the exception of St...
35. NIST AML-3.pdf (similarity: 0.522)
   Preview: With a focus on direct prompting attacks to enable misuse, we note the following broad categories of direct prompting techniques see 393:  Optimization-based attacks design attack objective functions ...
36. NIST AML-2.pdf (similarity: 0.521)
   Preview: 2.1.2. The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabil...
37. NIST AML-1.pdf (similarity: 0.520)
   Preview: The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities r...
38. NIST AML-5.pdf (similarity: 0.517)
   Preview: URL: https:arxiv.orgabs2402.18649, arXiv:2402.18649. 409 Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F. Naughton. A methodology for formalizing model-inversion attacks. In 2016 IEEE 29th Comput...
39. NIST AML-2.pdf (similarity: 0.517)
   Preview: The attack generalizes to all samples in a subpopulation and requires minimal knowledge about the ML model and a small number of poisoned samples proportional to the subpopulation size.
40. NIST AML-2.pdf (similarity: 0.517)
   Preview: Recently, poisoning attacks have gained more attention in industry applications as well 199. They can even be orchestrated at scale so that an adversary with limited financial resources could control ...